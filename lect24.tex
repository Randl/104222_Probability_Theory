Denote $\mu = \mathbb{E} W$
\paragraph{Claim}
$$\mathbb{E} X_n = \mu_n$$
\subparagraph{Proof}
For $n=1$:
$$\mathbb{E} X_1 = \mathbb{E}W = \mu$$
For $n>\geq 2$:
$$\mathbb{E} \left(X_n | X_{n-1} = m\right) = \mathbb{E}\sum_{i=1}^m W_i = \mu m $$
From smoothing theorem
$$\mathbb{E} \left( X_n | X_{n-1} \right) = \mu X_{n-1}$$
$$\mathbb{E} X_n = \mathbb{E} \mathbb{E} \left(X_n | X_{n-1}\right) = \mathbb{E} \mu X_{n-1} = \mu \mathbb{E} X_{n-1}$$
Thus $\mathbb{E} X_n = \mu^n$.
\paragraph{Conclusion}
$$\mu < 1 \Rightarrow P(S) = 0$$	
\subparagraph{Proof}
From Markov inequality
$$P(S_n) = P(X_n \geq 1) \leq \frac{1}{1} \mathbb{E} X_n = \mu^n \to 0$$
\paragraph{Claim}
\subparagraph{Proof}
Define $$\Phi(t) = \sum_{n=0}^\infty p_n t^n$$
,  probability generating function. Note that $\Phi(1) = \sum p_n  =1$.
By Taylor,
$$p_n =  \frac{\Phi^{(n)}(0)}{n!}$$
Since radius of convergence is at least 1, for $|t|<1$ we get
$$\Phi'(t) = \sum_{n=1}^\infty np_n t^{n-1}$$
$$\Phi''(t) = \sum_{n=2}^\infty n(n-1)p_n t^{n-2}$$
Then $\Phi'(t) \geq 0$ and $\Phi''(t) \geq 0$. If $p_0+p_1 < 1$ then $\Phi'(t) \geq 0 $ and $\Phi''(t)  > 0 $.

Thus $\Phi'(t)$ is monotonous increasing, and thus has limit.
\paragraph{Claim}
$$\lim_{t\to 1} \Phi'(t) = \mu$$
\subparagraph{Proof} 
$$\Phi'(t) = \sum_{n=1}^\infty np_n t^{n-1} \leq \sum_{n=1}^\infty np_n = \mu$$
On the other hand
$$\Phi'(t) \geq \sum_{n=1}^N np_n t^{n-1}$$
$$\lim_{t\to 1}\Phi'(t) \geq \lim_{t\to 1}\sum_{n=1}^N np_n t^{n-1} = \sum_{n=1}^N \lim_{t\to 1} np_n t^{n-1} = \sum_{n=1}^N np_n $$
\paragraph{Claim}
Suppose $p_0 > 0$ and $p_0+p_1 < 1$, then a smallest root $\alpha \in [0,1]$ of equation $\varphi(t) = t$ fulfills:
$$\begin{cases}
\alpha = 1 & \mu \leq 1 \\
\alpha \in (0,1) & \mu > 1
\end{cases}$$
Also if $\mu>1$ then $\Phi(t) > t$ for $t \in (0, \alpha)$ and $\Phi(t) < t $ for $t \in (\alpha, 1)$.
\subparagraph{Proof}
Denote $\psi(t) = \Phi(t) -t$. Then $\psi(0) = p_0$ and $\psi(1)=0$. Since $\psi^\prime$ is monotonically increasing, $\psi^\prime$ is monotonically increasing too. By previous claim, $$\lim_{t\to 1} \psi'(t) = \mu-1$$
If $\mu\leq1$, then 
$$\lim_{t \to 1} \psi'(t) \leq 0$$
Since $\psi^\prime$ is monotonically increasing, it's negative for $t \in [0,1)$.Since $\psi(1) = 0$ and derivative is negative, then $\psi(t) > 0 $ for  $t \in [0,1)$, and thus $\psi(t) > t$.

Now if $\mu > 1$, then $\mu-1>0$, i.e.
$$\lim_{t \to 1} \psi'(t) > 0$$
and exists neighborhood of 1 such that $\psi'(t) > 0$. Since $\psi(0)>0$ and $\psi(1) > 1$, and also $\Phi$ is convex, exists unique $\alpha \in (0,1)$ such that $\psi(\alpha)=0$. Moreover, 
$$\begin{cases}
\psi(s)>0 & s\in (0, \alpha) \\
\psi(s)<0 & s\in (\alpha,1) \\
\end{cases}$$
i.e.
$$\begin{cases}
\Phi(t)>t & t\in (0, \alpha) \\
\Phi(t)<t & t\in (\alpha,1) \\
\end{cases}$$
\paragraph{Theorem}
Suppose $p_0>0$. If $\mu \leq 1$ then $P(S)=0$. If $\mu> 1$ then $P(S) = \alpha > 0$, where $\alpha$ is smallest root of equation $\Phi(t)=t$.
\subparagraph{Proof}
If $p_0+p_1= 1$, then $\mu < 1$, and then we already proofed $P(S)=0$. Thus suppose $p_0+p_1 < 1$. Denote 
$$q = P\left(S^c\right)$$
and

$$q_n = P\left(S^c_n\right) = P(X_n=0)$$
We know that
$$q = \lim_{n\to \infty} q_n$$

Thus we want to show that if $\mu \leq 1$, then $q=1$ and if $\mu > 1$, $q=\alpha$.


\subparagraph{Claim}
$$q_n = \Phi\left(q_{n-1}\right)$$
\subparagraph{Proof}
For $n=1$, $q_1 = p_0$, and $\Phi(q_0) = \Phi(0) = p_0$.

Now, for $n\geq 2$:
$$P(X_n = 0 | X_1 = m) = q_{n-1}^m$$
Thus
$$q_n  = P(X_n= 0) = \sum_{m=0}^\infty P(X_n = 0 | X_1 = m) P(X_1 = m) = \sum_{m=0}^\infty p_m q_{n-1}^m = \Phi(q_{n-1})$$

\subparagraph{Proof of theorem (continued)}
Thus we get, in limit, that
$$\Phi(q) = q$$
From claim on roots of this equation we get that if $\mu \leq 1$, $q=1$. If $\mu > 1$ then $q \in \left\{ \alpha, 1 \right\}$. 

If $q=1$ then $q_n > \alpha $ for some big $n$. Then, from claim on roots of $\Phi(t)=t$, we get
 $$q_{n+1} = \Phi(q_n) < q_n$$
 which is contradiction, since $q$ is monotonically increasing. That means that $q=\alpha$.
 
 \section{The Probabilistic method}
 \subsection{Ramsey theory}
 Suppose we have full graph
 $K_n = \left(\left[ n \right], E \right)$, where $|E| = \binom{n}{2}$.
 \paragraph{2-coloring}
 Is coloring of graph's edges into two colors: red and blue.
 \paragraph{}
 $R(j) = R(j,j)$ is smallest number $R$ such that any 2-coloring of $K_R$ exists monochromatic clique $K_j$. Obviously $R(j) \leq \binom{2j-2}{j-1}\leq 4^{j-1}$
 \paragraph{Theorem (Erd\"{o}s)}
 $$R(j) \geq \frac{1}{e} \left( 1+o(1) \right) j \cdot 2^{\frac{j}{2}}$$ when $j \to \infty$.
 \subparagraph{Proof}
 Let $j \geq 3$. Choose random coloring of $K_n$, when each edge is colored independently from others with probability $\frac{1}{2}$ for each of colors.
 
 Let $W \subset K_n$ clique of size $j$. Define indicator 
 $$\mathds{1}_W = \begin{cases}
 1& W \text{ is monochromatic}\\
 0 & \text{otherwise}
 \end{cases} $$
 Then
 $$P\left(\mathds{1}_W = 1\right) = 2 \cdot \left( \frac{1}{2} \right)^{\binom{j}{2}} = 2^{1-\binom{j}{2}}$$
 Define
 $$X_j = \sum_{W\subset K_n: |W| = j} \mathds{1}_W$$
 which is number of monochromatic cliques of size $j$. The expectation of $X_j$:
 $$\mathbb{E} X_j = \binom{n}{j} 2^{1-\binom{j}{2}}$$
 
 Since average number of monochromatic cliques of size $j$ is $\binom{n}{j} 2^{1-\binom{j}{2}}$, exists coloring such that it has exactly $M$  monochromatic cliques of size $j$ for some $M$ such that $M \leq \binom{n}{j} 2^{1-\binom{j}{2}}$.
 
 For such coloring, remove one vertex for each of $M$ cliques. Denote by $M'$ number of removed vertices. $$M' \leq M \leq  \binom{n}{j} 2^{1-\binom{j}{2}}$$
 
 Take a look at full graph of $n-M'$ vertices. By construction, there is no monochromatic cliques of size $j$ in this graph. Thus
 $$R(j) > n-m \geq n - \binom{n}{j} 2^{1-\binom{j}{2}}$$
 In can be shown that
 $$\max \limits_{n: j \leq n < \infty} n - \binom{n}{j} 2^{1-\binom{j}{2}} = \frac{1}{e} \left( 1+o(1) \right) j \cdot 2^{\frac{j}{2}}$$
 \subsection{Disk Cover Problem}
 \paragraph{Theorem} For each choice of 10 points on plane $\left\{ x_i \right\}_{i=1}^{10}$ and each choice of $R>0$ it is always possible to cover points by non-overlapping disks of radius $R$. 